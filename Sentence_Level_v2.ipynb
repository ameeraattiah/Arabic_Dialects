{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mioJYTldU-qA"
   },
   "source": [
    "# Arabic Dialect Identification and Dialectness Scoring (Sentence-Level)\n",
    "*Done by:* Ameera Attiah"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g4wkVJaWUWMg"
   },
   "source": [
    "## Config & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: NVIDIA RTX A4500\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.set_device(1)                # use the second GPU\n",
    "torch.set_default_device(\"cuda:1\")      # make cuda:1 the default\n",
    "print(\"Using:\", torch.cuda.get_device_name(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fx-EZ0JZUF0K",
    "outputId": "9205a55a-8e73-4388-aa97-51389b5742f3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aattiah/Arabic_Dialects/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Configuration loaded\n",
      "📦 Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# 🧠 Step 1: Config & Imports\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset as TorchDataset, DataLoader\n",
    "from datasets import Dataset as HFDataset          # HuggingFace (aliased)\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    f1_score, mean_squared_error, mean_absolute_error,\n",
    "    classification_report, hamming_loss, confusion_matrix\n",
    ")\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"base_model\": \"UBC-NLP/MARBERT\",\n",
    "    \"num_labels\": 5,\n",
    "    \"batch_size\": 8,\n",
    "    \"num_epochs\": 3,\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"max_length\": 128,\n",
    "    \"classification_threshold\": 0.5,\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "}\n",
    "\n",
    "print(\"✅ Configuration loaded\")\n",
    "print(f\"📦 Using device: {config['device']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "Current GPU: NVIDIA RTX A4500\n",
      "GPU Memory (Allocated): 0.0 GB\n",
      "GPU Memory (Reserved): 0.0 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if GPU is available\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "\n",
    "# Check current device\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Current GPU:\", torch.cuda.get_device_name(0))\n",
    "    print(\"GPU Memory (Allocated):\", round(torch.cuda.memory_allocated(0) / 1024**3, 2), \"GB\")\n",
    "    print(\"GPU Memory (Reserved):\", round(torch.cuda.memory_reserved(0) / 1024**3, 2), \"GB\")\n",
    "else:\n",
    "    print(\"Using CPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "8OBsnrWIFjtN"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Arabic text cleaner\n",
    "def clean_arabic_text(text):\n",
    "    # Remove tatweel (ـ)\n",
    "    text = text.replace(\"ـ\", \"\")\n",
    "    # Reduce elongation: more than 2 repeated letters → keep only 2\n",
    "    text = re.sub(r'([\\u0621-\\u064A])\\1{2,}', r'\\1\\1', text)\n",
    "    # Normalize Arabic letters\n",
    "    text = re.sub(r'[إأآا]', 'ا', text)\n",
    "    text = re.sub(r'ى', 'ي', text)\n",
    "    text = re.sub(r'ؤ', 'و', text)\n",
    "    text = re.sub(r'ئ', 'ي', text)\n",
    "    text = re.sub(r'ة', 'ه', text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fAeJMgyaUaiF"
   },
   "source": [
    "## Load & Encode AOC-ALDi Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 103,
     "referenced_widgets": [
      "861f69ab7b184e368f4bb25095ef48b9",
      "26074db8d4454fa490e6652f8fe9f6bc",
      "6ef81a07d70e44d79e0815790ff9bf42",
      "77726d0a11ae4ae193f6bdc0d8a56176",
      "149238dd7ed54a1ca43c68688e3e7e2f",
      "7cab2d2b099c4cc4a498ee558741ae47",
      "41daca42de144559ad4afb3e2140b9a6",
      "0c4590aae9f8422d8d244d6468f70c6b",
      "128f9178b7094046901c7a538c3f31e6",
      "4b0bf0a7ec3e490cb696b9647a5d7e11",
      "cfb9e924ff994e69ab0f5990e8df4f51"
     ]
    },
    "id": "Sd1Ja7-6UPfb",
    "outputId": "107d67c3-3a70-451d-c9b5-ead176a6bd32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Loading AOC-ALDi dataset...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'load_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 13\u001b[0m\n\u001b[1;32m      4\u001b[0m DIALECT2IDX \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124megyptian\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlevantine\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmsa\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m4\u001b[39m\n\u001b[1;32m     10\u001b[0m }\n\u001b[1;32m     11\u001b[0m IDX2DIALECT \u001b[38;5;241m=\u001b[39m {i: d \u001b[38;5;28;01mfor\u001b[39;00m d, i \u001b[38;5;129;01min\u001b[39;00m DIALECT2IDX\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m---> 13\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marbml/AOC_ALDi\u001b[39m\u001b[38;5;124m\"\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mast\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mencode_labels\u001b[39m(row):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# 🧠 Step 2: Load & Encode Dataset\n",
    "\n",
    "print(\"📥 Loading AOC-ALDi dataset...\")\n",
    "DIALECT2IDX = {\n",
    "    \"egyptian\": 0,\n",
    "    \"levantine\": 1,\n",
    "    \"gulf\": 2,\n",
    "    \"maghrebi\": 3,\n",
    "    \"msa\": 4\n",
    "}\n",
    "IDX2DIALECT = {i: d for d, i in DIALECT2IDX.items()}\n",
    "\n",
    "dataset = load_dataset(\"arbml/AOC_ALDi\", split=\"train\")\n",
    "\n",
    "import ast\n",
    "\n",
    "def encode_labels(row):\n",
    "    multi_label = [0] * len(DIALECT2IDX)\n",
    "    regression = [0.0] * len(DIALECT2IDX)\n",
    "\n",
    "    dialects = row[\"dialect\"]\n",
    "    scores = row[\"dialectness_level\"]\n",
    "\n",
    "    # If they’re strings, convert them\n",
    "    if isinstance(dialects, str):\n",
    "        import ast\n",
    "        try:\n",
    "            dialects = ast.literal_eval(dialects)\n",
    "        except:\n",
    "            dialects = []\n",
    "    if isinstance(scores, str):\n",
    "        import ast\n",
    "        try:\n",
    "            scores = ast.literal_eval(scores)\n",
    "        except:\n",
    "            scores = []\n",
    "\n",
    "    # Now iterate safely\n",
    "    for d, s in zip(dialects, scores):\n",
    "        if d in DIALECT2IDX:\n",
    "            idx = DIALECT2IDX[d]\n",
    "            multi_label[idx] = 1\n",
    "            regression[idx] = float(s)\n",
    "\n",
    "    return {\n",
    "        \"multi_label\": multi_label,\n",
    "        \"regression\": regression\n",
    "    }\n",
    "# Step 1: Add a dominant dialect label (used for balancing)\n",
    "def extract_main_dialect(row):\n",
    "    # Choose the first dialect listed (most confident)\n",
    "    if isinstance(row[\"dialect\"], list):\n",
    "        return row[\"dialect\"][0]\n",
    "    elif isinstance(row[\"dialect\"], str) and row[\"dialect\"].startswith(\"[\"):\n",
    "        import ast\n",
    "        return ast.literal_eval(row[\"dialect\"])[0]\n",
    "    else:\n",
    "        return row[\"dialect\"]\n",
    "        \n",
    "processed_dataset = dataset.map(encode_labels)\n",
    "processed_dataset = processed_dataset.map(lambda row: {\"main_dialect\": extract_main_dialect(row)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE: make sure we always have a single main label index for CE training\n",
    "def extract_main_dialect_and_idx(row):\n",
    "    d = row[\"dialect\"]\n",
    "    if isinstance(d, list) and len(d) > 0:\n",
    "        main = d[0]\n",
    "    elif isinstance(d, str) and d.startswith(\"[\"):\n",
    "        main = ast.literal_eval(d)[0]\n",
    "    else:\n",
    "        main = d\n",
    "    main_idx = DIALECT2IDX.get(main, 4)  # default to MSA if weird\n",
    "    return {\"main_dialect\": main, \"main_idx\": main_idx}\n",
    "\n",
    "processed_dataset = dataset.map(encode_labels)\n",
    "processed_dataset = processed_dataset.map(extract_main_dialect_and_idx)\n",
    "\n",
    "# === Balancing (simple, keep your idea): upsample Maghrebi ===\n",
    "\n",
    "from random import choices\n",
    "\n",
    "maghrebi_rows = [row for row in processed_dataset if row[\"main_dialect\"] == \"maghrebi\"]\n",
    "non_maghrebi_rows = [row for row in processed_dataset if row[\"main_dialect\"] != \"maghrebi\"]\n",
    "\n",
    "avg_samples_per_class = len(non_maghrebi_rows) // 4 if len(non_maghrebi_rows) > 0 else len(maghrebi_rows)\n",
    "upsampled_maghrebi = choices(maghrebi_rows, k=avg_samples_per_class)\n",
    "\n",
    "balanced_rows = non_maghrebi_rows + upsampled_maghrebi\n",
    "balanced_dataset = HFDataset.from_list(balanced_rows)\n",
    "print(f\"✅ Balanced dataset size: {len(balanced_dataset)} (Maghrebi upsampled to {len(upsampled_maghrebi)})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE: simple train/val split (stratified-ish by shuffling per class)\n",
    "from collections import defaultdict\n",
    "by_cls = defaultdict(list)\n",
    "for row in balanced_dataset:\n",
    "    by_cls[row[\"main_idx\"]].append(row)\n",
    "\n",
    "train_rows, val_rows = [], []\n",
    "for _, rows in by_cls.items():\n",
    "    n = len(rows)\n",
    "    split = max(1, int(0.85 * n))\n",
    "    train_rows += rows[:split]\n",
    "    val_rows   += rows[split:]\n",
    "\n",
    "train_ds_raw = HFDataset.from_list(train_rows)\n",
    "val_ds_raw   = HFDataset.from_list(val_rows)\n",
    "print(f\"✅ Train/Val sizes: {len(train_ds_raw)} / {len(val_ds_raw)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0x7MhiE-UdwS"
   },
   "source": [
    "## Tokenizer + Custom Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DV9UJEvuUStX",
    "outputId": "82805026-58f0-4aa1-a4c2-edc2c08c4476"
   },
   "outputs": [],
   "source": [
    "# 🧠 Step 3: Tokenizer + Custom Dataset\n",
    "\n",
    "print(\"🔤 Loading MARBERT tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(config[\"base_model\"])\n",
    "\n",
    "# CHANGE: add 'label_ce' as a single int (main dialect) in __getitem__\n",
    "class AOC_Dataset(TorchDataset):\n",
    "    def __init__(self, dataset, tokenizer):\n",
    "        self.dataset = dataset          # <-- this is an HF dataset (HFDataset)\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataset[idx]\n",
    "\n",
    "        # pull the text; your processed HF rows must have \"sentence\", \"multi_label\", \"regression\"\n",
    "        text = row[\"sentence\"]\n",
    "        if isinstance(text, list):\n",
    "            text = \" \".join(text)\n",
    "\n",
    "        text = clean_arabic_text(text)\n",
    "        enc = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=config[\"max_length\"],\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": enc[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n",
    "            \"labels_cls\": torch.tensor(row[\"multi_label\"], dtype=torch.float),\n",
    "            \"labels_reg\": torch.tensor(row[\"regression\"], dtype=torch.float),\n",
    "        }\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "train_ds = AOC_Dataset(train_ds_raw, tokenizer)\n",
    "val_ds   = AOC_Dataset(val_ds_raw, tokenizer)\n",
    "\n",
    "print(\"✅ Tokenizer loaded and dataset class ready\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WcWH_6KJUhAG"
   },
   "source": [
    "## MARBERT Multi-Task Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZbSNh0kTUjrP",
    "outputId": "95a5cf2b-8d38-48a7-d80e-af269252271c"
   },
   "outputs": [],
   "source": [
    "# 🧠 Step 4: Multi-Task MARBERT Model\n",
    "\n",
    "print(\"🧠 Initializing MARBERT-based model...\")\n",
    "\n",
    "class BertForMultiTask(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(config[\"base_model\"])\n",
    "        hidden_size = self.bert.config.hidden_size\n",
    "        self.classifier = nn.Linear(hidden_size, config[\"num_labels\"])\n",
    "        self.regressor = nn.Linear(hidden_size, config[\"num_labels\"])\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        logits_cls = self.classifier(pooled_output)  # No sigmoid: BCEWithLogits handles that\n",
    "        logits_reg = torch.sigmoid(self.regressor(pooled_output))  # Output between 0–1\n",
    "        return logits_cls, logits_reg\n",
    "\n",
    "print(\"✅ Model class defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tV6A7TRMUmBM"
   },
   "source": [
    "## DataLoader, Model, Optimizer, and Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dlmyxHH_Upb_",
    "outputId": "fed6dcdb-25b6-4fd6-9941-d591def8cc56",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 🧠 Step 5: DataLoader, Model, Optimizer, Losses\n",
    "\n",
    "print(\"📦 Creating DataLoader...\")\n",
    "# CHANGE: make train/val loaders\n",
    "train_loader = DataLoader(train_ds, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=config[\"batch_size\"], shuffle=False)\n",
    "\n",
    "print(\"⚙️ Initializing model, optimizer, and loss functions...\")\n",
    "model = BertForMultiTask().to(config[\"device\"])\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config[\"learning_rate\"])\n",
    "\n",
    "# CHANGE: loss functions\n",
    "loss_fn_cls = nn.BCEWithLogitsLoss()\n",
    "loss_fn_reg = nn.MSELoss()\n",
    "lambda_reg = 0.2  # start small so classifier fixes itself\n",
    "\n",
    "b = next(iter(train_loader))\n",
    "print({k: v.shape if hasattr(v, \"shape\") else type(v) for k, v in b.items()})\n",
    "\n",
    "print(\"✅ Ready to train\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r9qCBePmUrpe"
   },
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "gKMhGc9jUuWB",
    "outputId": "730fde47-26d8-405a-a009-7cf7f23f2d7d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 🔁 Step 6: Training Loop (multi-label BCE + small reg weight)\n",
    "\n",
    "lambda_reg = 0.2  # <- keep regression, but don't let it dominate\n",
    "\n",
    "print(\"🚀 Starting training...\")\n",
    "for epoch in range(config[\"num_epochs\"]):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for step, batch in enumerate(train_loader):   # <-- use train_loader\n",
    "        input_ids = batch[\"input_ids\"].to(config[\"device\"])\n",
    "        attention_mask = batch[\"attention_mask\"].to(config[\"device\"])\n",
    "\n",
    "        # MULTI-LABEL targets (multi-hot)\n",
    "        labels_cls = batch[\"labels_cls\"].to(config[\"device\"])   # <— CHANGE: use labels_cls (multi-label)\n",
    "        labels_reg = batch[\"labels_reg\"].to(config[\"device\"])\n",
    "\n",
    "        # Forward\n",
    "        logits_cls, logits_reg = model(input_ids, attention_mask)\n",
    "\n",
    "        # Losses (BCE for multilabel; MSE for regression)\n",
    "        loss_cls = loss_fn_cls(logits_cls, labels_cls)          # <— CHANGE: BCEWithLogitsLoss on multi-hot\n",
    "        loss_reg = loss_fn_reg(logits_reg, labels_reg)\n",
    "        loss = loss_cls + lambda_reg * loss_reg\n",
    "\n",
    "        # Backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if step % 50 == 0:\n",
    "            print(f\"🟡 Epoch {epoch+1} Step {step}/{len(train_loader)} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = total_loss / max(1, len(train_loader))\n",
    "    print(f\"✅ Epoch {epoch+1} | Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # ---- quick MULTI-LABEL validation on val_loader ----\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:  # <-- use val_loader\n",
    "            ids  = batch[\"input_ids\"].to(config[\"device\"])\n",
    "            mask = batch[\"attention_mask\"].to(config[\"device\"])\n",
    "\n",
    "            logits_cls, _ = model(ids, mask)\n",
    "            probs = torch.sigmoid(logits_cls).cpu().numpy()  # (B, 5)\n",
    "            preds = (probs >= config[\"classification_threshold\"]).astype(int)\n",
    "\n",
    "            y_true.append(batch[\"labels_cls\"].cpu().numpy())\n",
    "            y_pred.append(preds)\n",
    "\n",
    "    y_true = np.vstack(y_true)\n",
    "    y_pred = np.vstack(y_pred)\n",
    "    print(classification_report(y_true, y_pred, target_names=list(DIALECT2IDX.keys()), digits=3))\n",
    "\n",
    "# Save\n",
    "torch.save(model.state_dict(), \"marbert_sentence_model2.pt\")\n",
    "print(\"💾 Model saved to marbert_sentence_model2.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5kb6eN32Uwub"
   },
   "source": [
    "## Evaluation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n4zv_R2tU0SM"
   },
   "outputs": [],
   "source": [
    "# 🧪 Step 7: Evaluation Loop (multi-label, on VAL set)\n",
    "\n",
    "print(\"🔍 Running evaluation...\")\n",
    "model.eval()\n",
    "\n",
    "all_preds_cls, all_labels_cls = [], []\n",
    "all_preds_reg, all_labels_reg = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:  # <-- use val_loader, not 'dataloader'\n",
    "        input_ids = batch[\"input_ids\"].to(config[\"device\"])\n",
    "        attention_mask = batch[\"attention_mask\"].to(config[\"device\"])\n",
    "\n",
    "        # Ground-truth\n",
    "        labels_cls = batch[\"labels_cls\"].cpu().numpy()\n",
    "        labels_reg = batch[\"labels_reg\"].cpu().numpy()\n",
    "\n",
    "        # Forward pass\n",
    "        logits_cls, logits_reg = model(input_ids, attention_mask)\n",
    "\n",
    "        # MULTI-LABEL predictions (sigmoid + threshold)\n",
    "        probs_cls = torch.sigmoid(logits_cls).cpu().numpy()\n",
    "        preds_cls = (probs_cls >= config[\"classification_threshold\"]).astype(int)\n",
    "\n",
    "        # Regression outputs (your reg head already uses sigmoid in the model)\n",
    "        preds_reg = logits_reg.cpu().numpy()\n",
    "\n",
    "        # Collect\n",
    "        all_preds_cls.append(preds_cls)\n",
    "        all_labels_cls.append(labels_cls)\n",
    "        all_preds_reg.append(preds_reg)\n",
    "        all_labels_reg.append(labels_reg)\n",
    "\n",
    "print(\"✅ Evaluation complete\")\n",
    "\n",
    "# Stack to arrays for metrics in block 3\n",
    "all_preds_cls  = np.vstack(all_preds_cls)\n",
    "all_labels_cls = np.vstack(all_labels_cls)\n",
    "all_preds_reg  = np.vstack(all_preds_reg)\n",
    "all_labels_reg = np.vstack(all_labels_reg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6h7JXfrOU2wh"
   },
   "source": [
    "## Full Metrics & Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RbJT7p85U5X-"
   },
   "outputs": [],
   "source": [
    "# 📊 Step 8: Full Metrics + Visualizations (multi-label)\n",
    "\n",
    "print(\"📊 Generating metrics and plots...\")\n",
    "\n",
    "# ---- Classification (multi-label) ----\n",
    "print(\"\\n📄 Classification Report:\")\n",
    "print(classification_report(all_labels_cls, all_preds_cls, target_names=list(DIALECT2IDX.keys()), digits=3))\n",
    "print(\"✅ Micro F1:\", f1_score(all_labels_cls, all_preds_cls, average='micro'))\n",
    "print(\"✅ Macro F1:\", f1_score(all_labels_cls, all_preds_cls, average='macro'))\n",
    "print(\"📉 Hamming Loss:\", hamming_loss(all_labels_cls, all_preds_cls))\n",
    "\n",
    "# ---- Regression ----\n",
    "print(\"\\n📈 Regression Metrics:\")\n",
    "mse = mean_squared_error(all_labels_reg, all_preds_reg)\n",
    "mae = mean_absolute_error(all_labels_reg, all_preds_reg)\n",
    "print(f\"✅ MSE: {mse:.4f}\")\n",
    "print(f\"✅ MAE: {mae:.4f}\")\n",
    "\n",
    "# Pearson and Spearman per dialect\n",
    "for i, dialect in enumerate(DIALECT2IDX.keys()):\n",
    "    pearson = pearsonr(all_labels_reg[:, i], all_preds_reg[:, i])[0]\n",
    "    spearman = spearmanr(all_labels_reg[:, i], all_preds_reg[:, i])[0]\n",
    "    print(f\"{dialect:10} | Pearson: {pearson:.4f} | Spearman: {spearman:.4f}\")\n",
    "\n",
    "# ---- Per-dialect confusion matrices (multi-label: 2x2 per class) ----\n",
    "print(\"\\n📊 Confusion Matrices (per dialect, multi-label):\")\n",
    "for i, dialect in enumerate(DIALECT2IDX.keys()):\n",
    "    y_true_bin = all_labels_cls[:, i].astype(int)\n",
    "    y_pred_bin = all_preds_cls[:, i].astype(int)\n",
    "    cm = confusion_matrix(y_true_bin, y_pred_bin, labels=[0,1])\n",
    "    plt.figure(figsize=(3, 3))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\",\n",
    "                xticklabels=[\"Pred 0\", \"Pred 1\"], yticklabels=[\"True 0\", \"True 1\"])\n",
    "    plt.title(f\"Confusion Matrix: {dialect}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ---- Regression scatter ----\n",
    "print(\"📈 Plotting regression scatter plot...\")\n",
    "plt.figure(figsize=(7, 5))\n",
    "for i, dialect in enumerate(DIALECT2IDX.keys()):\n",
    "    plt.scatter(all_labels_reg[:, i], all_preds_reg[:, i], label=dialect, alpha=0.5)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel(\"True Dialectness Score\")\n",
    "plt.ylabel(\"Predicted Score\")\n",
    "plt.title(\"Dialectness Regression: True vs Predicted\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6dkOPXovQ2FV"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "# Inference on the Fineweb2 dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1bkXvadtRSUW"
   },
   "source": [
    "##  Load Your Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AmqcaG6gRQcB",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# 1️⃣ Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"UBC-NLP/MARBERT\")\n",
    "\n",
    "# 2️⃣ Initialize model architecture\n",
    "model = BertForMultiTask()\n",
    "\n",
    "# 3️⃣ Load weights\n",
    "model.load_state_dict(torch.load(\"marbert_sentence_model2.pt\", map_location=\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "# 4️⃣ Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UqC-wcNqGjZt"
   },
   "outputs": [],
   "source": [
    "# gulf dialect\n",
    "# test_sentence = \"\"\"\n",
    "# وش السالفة يا ولد؟ أنا امس كنت رايح للبر مع الربع، وشبّينا النار وسوينا قهوة وشاي، وبدينا نسولف عن أيام الطيبين.\n",
    "# مر الوقت بسرعة، وكل واحد قام يحكي عن مواقفه يوم كان بالثانوية. بعدين طقينا العشاء رز ولحم، ولا أحلى.\n",
    "# رجعنا للبيت قبل الفجر، تعبانين لكن مبسوطين، ما في مثل جمعة الربع بالبر.\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "# egyptian dialect\n",
    "# test_sentence = \"\"\"\n",
    "# أنا بصراحة زهقت من الزحمة والعياط بتاع كل يوم في الشغل، يعني مافيش يوم بيعدّي من غير ما المواصلات تعمل فيا مقلب!\n",
    "# النهاردة مثلاً، صحيت متأخر علشان الكهربا قطعت، ونزلت أجري على الميكروباص، بس السواق قرر ياخد اللفة كلها قبل ما يوصلني.\n",
    "# عدّينا من عند الكوبري اللي دايمًا زحمة، والناس قاعدة بتزعق لبعض من غير سبب، وكأن العصبية بقت طبيعية عندنا.\n",
    "# بعد كده وصلت الشغل، والمدير أول ما شافني قالي: \"اتأخرت تاني؟\" وأنا بصيت له وقلت: \"والله غصب عني\".\n",
    "# يعني هو فاكر إننا بنحب التأخير؟ هو مش حاسس باللي بنشوفه كل يوم في الشارع؟\n",
    "# المهم، يوم عدى ويا عالم بكرا هيبقى فيه إيه!\n",
    "# \"\"\"\n",
    "\n",
    "# levantine dialect\n",
    "# test_sentence = \"\"\"\n",
    "# مبارح نزلت عالسوق مع رفيقتي، وكان في كتير عجقة بس الجو كان حلو. اشترينا شوية خضرة وفواكه، وبعدين رحنا نشرب قهوة بشارع الحمرا.\n",
    "# قعدنا شي ساعة، ضحكنا وحكينا عن الشغل والحياة. بعدين إجا أخوها بسيارته وأخدنا عالبيت. عنجد كان نهار كتير مهضوم.\n",
    "# \"\"\"\n",
    "\n",
    "# maghrabi dialect\n",
    "test_sentence = \"\"\"\n",
    "اليوم بكري مشيت للمارشي نشري شوية خضرة، لقيت الدنيا عامرة والناس كيتسابقو باش يشرو قبل ما تسالي الصباح.\n",
    "شريت مطيشة، بصلة، وشي شوية ديال النعناع. ومن بعد مشيت عند الجارة نشربو أتاي وندويو على ولادنا.\n",
    "الدنيا زوينة ولكن خاصنا نتهلاو فبعضياتنا.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "cleaned_sentence = clean_arabic_text(test_sentence)\n",
    "\n",
    "encoded = tokenizer(cleaned_sentence, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "\n",
    "# ✅ Fix for unexpected keyword\n",
    "if \"token_type_ids\" in encoded:\n",
    "    del encoded[\"token_type_ids\"]\n",
    "\n",
    "encoded = {k: v.to(device) for k, v in encoded.items()}\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits_cls, logits_reg = model(**encoded)\n",
    "    probs_cls = torch.sigmoid(logits_cls).cpu().numpy()[0]\n",
    "    preds_cls = (probs_cls > 0.5).astype(int)\n",
    "    preds_reg = logits_reg.cpu().numpy()[0]\n",
    "\n",
    "DIALECTS = [\"egyptian\", \"levantine\", \"gulf\", \"maghrebi\", \"msa\"]\n",
    "predicted_dialects = [DIALECTS[i] for i, v in enumerate(preds_cls) if v == 1]\n",
    "\n",
    "print(\"📝 Input:\", test_sentence)\n",
    "print(\"🧼 Cleaned:\", cleaned_sentence)\n",
    "print(\"🧠 Predicted Dialects:\", predicted_dialects)\n",
    "for i, d in enumerate(DIALECTS):\n",
    "    print(f\" - {d}: {preds_reg[i]:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5A-NYCXfRUr7"
   },
   "source": [
    "## Load & Preprocess FineWeb2 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vRsCdxArRXUH",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# ✅ All Arabic dialect subsets in FineWeb2\n",
    "dialects = [\n",
    "    \"acm_Arab\",  # Iraqi\n",
    "    # \"aeb_Arab\",  # Tunisian\n",
    "    \"apc_Arab\",  # Levantine\n",
    "    \"arb_Arab\",  # MSA\n",
    "    \"arq_Arab\",  # Algerian\n",
    "    \"ars_Arab\",  # Najdi (Saudi)\n",
    "    \"ary_Arab\",  # Moroccan\n",
    "    \"arz_Arab\",  # Egyptian\n",
    "    \"ayp_Arab\",  # North Mesopotamian\n",
    "    \"shu_Arab\",  # Chadian/Sudanese\n",
    "]\n",
    "\n",
    "# 📦 Stream & sample texts from each dialect\n",
    "samples = []\n",
    "samples_per_dialect = 100  # adjust as needed\n",
    "\n",
    "for dialect in dialects:\n",
    "    print(f\"📥 Sampling from: {dialect}\")\n",
    "    try:\n",
    "        dataset = load_dataset(\"HuggingFaceFW/fineweb-2\", dialect, split=\"train\", streaming=True)\n",
    "        for i, sample in enumerate(dataset):\n",
    "            samples.append(sample[\"text\"])\n",
    "            if i + 1 >= samples_per_dialect:\n",
    "                break\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading {dialect}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JnwrMprPRZIk"
   },
   "source": [
    "## Tokenize the Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-TGktbEQRbPL",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "encoded_batch = tokenizer(samples, padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoded_batch = {k: v.to(device) for k, v in encoded_batch.items()}\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uQE9KD06RcwV"
   },
   "source": [
    "## Run the Model for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6M29vJzKRfAW"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    logits_cls, logits_reg = model(encoded_batch[\"input_ids\"], encoded_batch[\"attention_mask\"])\n",
    "\n",
    "# Convert logits to predictions\n",
    "probs_cls = torch.sigmoid(logits_cls).cpu().numpy()\n",
    "preds_reg = logits_reg.cpu().numpy()\n",
    "\n",
    "# Threshold classification (multi-label)\n",
    "import numpy as np\n",
    "preds_cls = (probs_cls > 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b904tAhVRgii"
   },
   "source": [
    "## Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h9FwowQyRinq",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DIALECTS = [\"egyptian\", \"levantine\", \"gulf\", \"maghrebi\", \"msa\"]\n",
    "\n",
    "for sentence, cls, reg in zip(samples, preds_cls, preds_reg):\n",
    "    sentence = clean_arabic_text(sentence)\n",
    "    predicted_dialects = [DIALECTS[j] for j, val in enumerate(cls) if val == 1]\n",
    "    regression_scores = dict(zip(DIALECTS, reg))\n",
    "\n",
    "    print(\"📝 Text:\", sentence[:60])\n",
    "    print(\"🧠 Predicted Dialects:\", predicted_dialects)\n",
    "    for d in DIALECTS:\n",
    "        print(f\" - {d}: {regression_scores[d]:.3f}\")\n",
    "    print(\"=\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "\n",
    "# Check current device\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Current GPU:\", torch.cuda.get_device_name(0))\n",
    "    print(\"GPU Memory (Allocated):\", round(torch.cuda.memory_allocated(0) / 1024**3, 2), \"GB\")\n",
    "    print(\"GPU Memory (Reserved):\", round(torch.cuda.memory_reserved(0) / 1024**3, 2), \"GB\")\n",
    "else:\n",
    "    print(\"Using CPU\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference on Synthetic Data \n",
    "made using ChatGPT, 100 document samples of each dialect (egyptian, levantine, gulf, maghrabi, msa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import torch\n",
    "import re\n",
    "from random import sample as random_sample\n",
    "\n",
    "# ✅ Define dialect mappings\n",
    "DIALECTS = [\"egyptian\", \"levantine\", \"gulf\", \"maghrebi\", \"msa\"]\n",
    "DIALECT2IDX = {d: i for i, d in enumerate(DIALECTS)}\n",
    "IDX2DIALECT = {i: d for i, d in enumerate(DIALECTS)}\n",
    "\n",
    "# 🧠 Re-initialize the model architecture\n",
    "model = BertForMultiTask().to(config[\"device\"])\n",
    "\n",
    "# 🔁 Load trained weights\n",
    "checkpoint_path = \"marbert_sentence_model2.pt\"\n",
    "model.load_state_dict(torch.load(checkpoint_path, map_location=config[\"device\"]))\n",
    "\n",
    "print(\"✅ Model loaded from checkpoint.\")\n",
    "\n",
    "# 📥 Load JSON\n",
    "with open(\"dialect_documents.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    dialect_docs = json.load(f)\n",
    "\n",
    "# 🧪 Flatten all samples\n",
    "test_samples = []\n",
    "for label, docs in dialect_docs.items():\n",
    "    for doc in docs:\n",
    "        test_samples.append({\"text\": doc[\"text\"], \"label\": label})\n",
    "\n",
    "def split_sentences(text):\n",
    "    \"\"\"\n",
    "    Splits the input Arabic text into sentences using simple punctuation heuristics.\n",
    "    \"\"\"\n",
    "    # Normalize spacing and remove excessive newlines\n",
    "    text = re.sub(r'\\s+', ' ', text.strip())\n",
    "    \n",
    "    # Split on Arabic and standard sentence-ending punctuation\n",
    "    sentence_enders = re.compile(r'(?<=[.!؟\\n])\\s+')\n",
    "    sentences = sentence_enders.split(text)\n",
    "    \n",
    "    # Remove empty or very short entries\n",
    "    return [s.strip() for s in sentences if len(s.strip()) > 5]\n",
    "\n",
    "# ✅ Predict sentence-wise\n",
    "def predict_doc_label_and_score(text):\n",
    "    model.eval()\n",
    "    sentences = split_sentences(text)\n",
    "    sentence_preds = []\n",
    "    sentence_scores = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence = clean_arabic_text(sentence)\n",
    "        encoded = tokenizer(\n",
    "            sentence,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=config[\"max_length\"],\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(config[\"device\"])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits_cls, logits_reg = model(encoded[\"input_ids\"], encoded[\"attention_mask\"])\n",
    "            \n",
    "            # Keep probabilities for ALL classes (multi-label)\n",
    "            probs_cls = torch.sigmoid(logits_cls).cpu().numpy()[0]  # shape (5,)\n",
    "            \n",
    "            # Instead of one-hot argmax, keep full probabilities\n",
    "            sentence_preds.append(probs_cls)\n",
    "            \n",
    "            # Regression as before\n",
    "            pred_reg = torch.sigmoid(logits_reg).cpu().numpy()[0]\n",
    "            sentence_scores.append(pred_reg)\n",
    "\n",
    "\n",
    "    # ✅ Aggregate both classification (votes) and regression (avg score)\n",
    "    votes = np.sum(sentence_preds, axis=0)\n",
    "    avg_reg = np.mean(sentence_scores, axis=0)\n",
    "\n",
    "    boost = np.ones(len(DIALECTS))\n",
    "    boost[DIALECT2IDX[\"maghrebi\"]] = 5.0  # strong boost\n",
    "    boost[DIALECT2IDX[\"gulf\"]] = 1.6      # optional\n",
    "    \n",
    "    combined = (0.75 * votes + 0.25 * avg_reg) * boost\n",
    "    \n",
    "\n",
    "    pred_idx = np.argmax(combined)\n",
    "    pred_label = IDX2DIALECT[pred_idx]\n",
    "\n",
    "    return pred_label, avg_reg.tolist()\n",
    "\n",
    "# 🚀 Predict all\n",
    "predictions = []\n",
    "for sample in tqdm(test_samples, desc=\"🔍 Predicting dialects\"):\n",
    "    text = sample[\"text\"]\n",
    "    true = sample[\"label\"]\n",
    "    pred, scores = predict_doc_label_and_score(text)\n",
    "    predictions.append({\n",
    "        \"text\": text,\n",
    "        \"true\": true,\n",
    "        \"pred\": pred,\n",
    "        \"scores\": scores\n",
    "    })\n",
    "\n",
    "# 📌 Show 5 samples per dialect\n",
    "grouped = defaultdict(list)\n",
    "for p in predictions:\n",
    "    grouped[p[\"true\"]].append(p)\n",
    "\n",
    "print(\"\\n📌 Sample Predictions (5 per dialect):\")\n",
    "for d in DIALECTS:\n",
    "    print(f\"\\n=== {d.upper()} ===\")\n",
    "    for s in random_sample(grouped[d], 1):\n",
    "        snippet = s[\"text\"][:150].replace('\\n', ' ')\n",
    "        print(f\"True: {s['true']:<10} | Pred: {s['pred']:<10} | Scores: {np.round(s['scores'], 2)}\")\n",
    "        print(f\"Text: {snippet}...\\n\")\n",
    "\n",
    "# 📊 Accuracy\n",
    "summary = defaultdict(lambda: {\"correct\": 0, \"wrong\": 0})\n",
    "for p in predictions:\n",
    "    if p[\"true\"] == p[\"pred\"]:\n",
    "        summary[p[\"true\"]][\"correct\"] += 1\n",
    "    else:\n",
    "        summary[p[\"true\"]][\"wrong\"] += 1\n",
    "\n",
    "print(\"\\n📊 Dialect Classification Accuracy:\\n\")\n",
    "print(f\"{'Dialect':<12} {'Correct':>7} {'Wrong':>7} {'Total':>7} {'Accuracy':>9}\")\n",
    "print(\"-\" * 45)\n",
    "for d in DIALECTS:\n",
    "    correct = summary[d][\"correct\"]\n",
    "    wrong = summary[d][\"wrong\"]\n",
    "    total = correct + wrong\n",
    "    acc = 100 * correct / total if total > 0 else 0\n",
    "    print(f\"{d:<12} {correct:>7} {wrong:>7} {total:>7} {acc:>8.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the dialect labels (must match your data)\n",
    "dialects = [\"egyptian\", \"levantine\", \"gulf\", \"maghrebi\", \"msa\"]\n",
    "\n",
    "# Extract true and predicted labels from your prediction results\n",
    "y_true = [p[\"true\"] for p in predictions]\n",
    "y_pred = [p[\"pred\"] for p in predictions]\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred, labels=dialects)\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=dialects, yticklabels=dialects)\n",
    "plt.xlabel(\"Predicted Dialect\")\n",
    "plt.ylabel(\"Actual Dialect\")\n",
    "plt.title(\"Dialect Classification Confusion Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n❌ Misclassified Samples:\\n\")\n",
    "\n",
    "for item in predictions:\n",
    "    if item[\"true\"] != item[\"pred\"]:\n",
    "        text_snippet = item[\"text\"][:200].replace(\"\\n\", \" \")\n",
    "        print(f\"True: {item['true']:<10} | Predicted: {item['pred']:<10} | Text: {text_snippet}...\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (Arabic Dialects)",
   "language": "python",
   "name": "arabic-dialects"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0c4590aae9f8422d8d244d6468f70c6b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "128f9178b7094046901c7a538c3f31e6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "149238dd7ed54a1ca43c68688e3e7e2f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "26074db8d4454fa490e6652f8fe9f6bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7cab2d2b099c4cc4a498ee558741ae47",
      "placeholder": "​",
      "style": "IPY_MODEL_41daca42de144559ad4afb3e2140b9a6",
      "value": "Map: 100%"
     }
    },
    "41daca42de144559ad4afb3e2140b9a6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4b0bf0a7ec3e490cb696b9647a5d7e11": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6ef81a07d70e44d79e0815790ff9bf42": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0c4590aae9f8422d8d244d6468f70c6b",
      "max": 102886,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_128f9178b7094046901c7a538c3f31e6",
      "value": 102886
     }
    },
    "77726d0a11ae4ae193f6bdc0d8a56176": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4b0bf0a7ec3e490cb696b9647a5d7e11",
      "placeholder": "​",
      "style": "IPY_MODEL_cfb9e924ff994e69ab0f5990e8df4f51",
      "value": " 102886/102886 [00:27&lt;00:00, 4147.65 examples/s]"
     }
    },
    "7cab2d2b099c4cc4a498ee558741ae47": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "861f69ab7b184e368f4bb25095ef48b9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_26074db8d4454fa490e6652f8fe9f6bc",
       "IPY_MODEL_6ef81a07d70e44d79e0815790ff9bf42",
       "IPY_MODEL_77726d0a11ae4ae193f6bdc0d8a56176"
      ],
      "layout": "IPY_MODEL_149238dd7ed54a1ca43c68688e3e7e2f"
     }
    },
    "cfb9e924ff994e69ab0f5990e8df4f51": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
