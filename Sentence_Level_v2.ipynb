{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mioJYTldU-qA"
   },
   "source": [
    "# Arabic Dialect Identification and Dialectness Scoring (Sentence-Level)\n",
    "*Done by:* Ameera Attiah"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g4wkVJaWUWMg"
   },
   "source": [
    "## Config & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: NVIDIA RTX A4500\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.set_device(1)                # use the second GPU\n",
    "torch.set_default_device(\"cuda:1\")      # make cuda:1 the default\n",
    "print(\"Using:\", torch.cuda.get_device_name(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fx-EZ0JZUF0K",
    "outputId": "9205a55a-8e73-4388-aa97-51389b5742f3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aattiah/Arabic_Dialects/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration loaded\n",
      "üì¶ Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# üß† Step 1: Config & Imports\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset as TorchDataset, DataLoader\n",
    "from datasets import Dataset as HFDataset          # HuggingFace (aliased)\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    f1_score, mean_squared_error, mean_absolute_error,\n",
    "    classification_report, hamming_loss, confusion_matrix\n",
    ")\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"base_model\": \"UBC-NLP/MARBERT\",\n",
    "    \"num_labels\": 5,\n",
    "    \"batch_size\": 8,\n",
    "    \"num_epochs\": 3,\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"max_length\": 128,\n",
    "    \"classification_threshold\": 0.5,\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Configuration loaded\")\n",
    "print(f\"üì¶ Using device: {config['device']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "Current GPU: NVIDIA RTX A4500\n",
      "GPU Memory (Allocated): 0.0 GB\n",
      "GPU Memory (Reserved): 0.0 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if GPU is available\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "\n",
    "# Check current device\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Current GPU:\", torch.cuda.get_device_name(0))\n",
    "    print(\"GPU Memory (Allocated):\", round(torch.cuda.memory_allocated(0) / 1024**3, 2), \"GB\")\n",
    "    print(\"GPU Memory (Reserved):\", round(torch.cuda.memory_reserved(0) / 1024**3, 2), \"GB\")\n",
    "else:\n",
    "    print(\"Using CPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "8OBsnrWIFjtN"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Arabic text cleaner\n",
    "def clean_arabic_text(text):\n",
    "    # Remove tatweel (ŸÄ)\n",
    "    text = text.replace(\"ŸÄ\", \"\")\n",
    "    # Reduce elongation: more than 2 repeated letters ‚Üí keep only 2\n",
    "    text = re.sub(r'([\\u0621-\\u064A])\\1{2,}', r'\\1\\1', text)\n",
    "    # Normalize Arabic letters\n",
    "    text = re.sub(r'[ÿ•ÿ£ÿ¢ÿß]', 'ÿß', text)\n",
    "    text = re.sub(r'Ÿâ', 'Ÿä', text)\n",
    "    text = re.sub(r'ÿ§', 'Ÿà', text)\n",
    "    text = re.sub(r'ÿ¶', 'Ÿä', text)\n",
    "    text = re.sub(r'ÿ©', 'Ÿá', text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fAeJMgyaUaiF"
   },
   "source": [
    "## Load & Encode AOC-ALDi Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 103,
     "referenced_widgets": [
      "861f69ab7b184e368f4bb25095ef48b9",
      "26074db8d4454fa490e6652f8fe9f6bc",
      "6ef81a07d70e44d79e0815790ff9bf42",
      "77726d0a11ae4ae193f6bdc0d8a56176",
      "149238dd7ed54a1ca43c68688e3e7e2f",
      "7cab2d2b099c4cc4a498ee558741ae47",
      "41daca42de144559ad4afb3e2140b9a6",
      "0c4590aae9f8422d8d244d6468f70c6b",
      "128f9178b7094046901c7a538c3f31e6",
      "4b0bf0a7ec3e490cb696b9647a5d7e11",
      "cfb9e924ff994e69ab0f5990e8df4f51"
     ]
    },
    "id": "Sd1Ja7-6UPfb",
    "outputId": "107d67c3-3a70-451d-c9b5-ead176a6bd32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Loading AOC-ALDi dataset...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'load_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 13\u001b[0m\n\u001b[1;32m      4\u001b[0m DIALECT2IDX \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124megyptian\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlevantine\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmsa\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m4\u001b[39m\n\u001b[1;32m     10\u001b[0m }\n\u001b[1;32m     11\u001b[0m IDX2DIALECT \u001b[38;5;241m=\u001b[39m {i: d \u001b[38;5;28;01mfor\u001b[39;00m d, i \u001b[38;5;129;01min\u001b[39;00m DIALECT2IDX\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m---> 13\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marbml/AOC_ALDi\u001b[39m\u001b[38;5;124m\"\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mast\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mencode_labels\u001b[39m(row):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# üß† Step 2: Load & Encode Dataset\n",
    "\n",
    "print(\"üì• Loading AOC-ALDi dataset...\")\n",
    "DIALECT2IDX = {\n",
    "    \"egyptian\": 0,\n",
    "    \"levantine\": 1,\n",
    "    \"gulf\": 2,\n",
    "    \"maghrebi\": 3,\n",
    "    \"msa\": 4\n",
    "}\n",
    "IDX2DIALECT = {i: d for d, i in DIALECT2IDX.items()}\n",
    "\n",
    "dataset = load_dataset(\"arbml/AOC_ALDi\", split=\"train\")\n",
    "\n",
    "import ast\n",
    "\n",
    "def encode_labels(row):\n",
    "    multi_label = [0] * len(DIALECT2IDX)\n",
    "    regression = [0.0] * len(DIALECT2IDX)\n",
    "\n",
    "    dialects = row[\"dialect\"]\n",
    "    scores = row[\"dialectness_level\"]\n",
    "\n",
    "    # If they‚Äôre strings, convert them\n",
    "    if isinstance(dialects, str):\n",
    "        import ast\n",
    "        try:\n",
    "            dialects = ast.literal_eval(dialects)\n",
    "        except:\n",
    "            dialects = []\n",
    "    if isinstance(scores, str):\n",
    "        import ast\n",
    "        try:\n",
    "            scores = ast.literal_eval(scores)\n",
    "        except:\n",
    "            scores = []\n",
    "\n",
    "    # Now iterate safely\n",
    "    for d, s in zip(dialects, scores):\n",
    "        if d in DIALECT2IDX:\n",
    "            idx = DIALECT2IDX[d]\n",
    "            multi_label[idx] = 1\n",
    "            regression[idx] = float(s)\n",
    "\n",
    "    return {\n",
    "        \"multi_label\": multi_label,\n",
    "        \"regression\": regression\n",
    "    }\n",
    "# Step 1: Add a dominant dialect label (used for balancing)\n",
    "def extract_main_dialect(row):\n",
    "    # Choose the first dialect listed (most confident)\n",
    "    if isinstance(row[\"dialect\"], list):\n",
    "        return row[\"dialect\"][0]\n",
    "    elif isinstance(row[\"dialect\"], str) and row[\"dialect\"].startswith(\"[\"):\n",
    "        import ast\n",
    "        return ast.literal_eval(row[\"dialect\"])[0]\n",
    "    else:\n",
    "        return row[\"dialect\"]\n",
    "        \n",
    "processed_dataset = dataset.map(encode_labels)\n",
    "processed_dataset = processed_dataset.map(lambda row: {\"main_dialect\": extract_main_dialect(row)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE: make sure we always have a single main label index for CE training\n",
    "def extract_main_dialect_and_idx(row):\n",
    "    d = row[\"dialect\"]\n",
    "    if isinstance(d, list) and len(d) > 0:\n",
    "        main = d[0]\n",
    "    elif isinstance(d, str) and d.startswith(\"[\"):\n",
    "        main = ast.literal_eval(d)[0]\n",
    "    else:\n",
    "        main = d\n",
    "    main_idx = DIALECT2IDX.get(main, 4)  # default to MSA if weird\n",
    "    return {\"main_dialect\": main, \"main_idx\": main_idx}\n",
    "\n",
    "processed_dataset = dataset.map(encode_labels)\n",
    "processed_dataset = processed_dataset.map(extract_main_dialect_and_idx)\n",
    "\n",
    "# === Balancing (simple, keep your idea): upsample Maghrebi ===\n",
    "\n",
    "from random import choices\n",
    "\n",
    "maghrebi_rows = [row for row in processed_dataset if row[\"main_dialect\"] == \"maghrebi\"]\n",
    "non_maghrebi_rows = [row for row in processed_dataset if row[\"main_dialect\"] != \"maghrebi\"]\n",
    "\n",
    "avg_samples_per_class = len(non_maghrebi_rows) // 4 if len(non_maghrebi_rows) > 0 else len(maghrebi_rows)\n",
    "upsampled_maghrebi = choices(maghrebi_rows, k=avg_samples_per_class)\n",
    "\n",
    "balanced_rows = non_maghrebi_rows + upsampled_maghrebi\n",
    "balanced_dataset = HFDataset.from_list(balanced_rows)\n",
    "print(f\"‚úÖ Balanced dataset size: {len(balanced_dataset)} (Maghrebi upsampled to {len(upsampled_maghrebi)})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE: simple train/val split (stratified-ish by shuffling per class)\n",
    "from collections import defaultdict\n",
    "by_cls = defaultdict(list)\n",
    "for row in balanced_dataset:\n",
    "    by_cls[row[\"main_idx\"]].append(row)\n",
    "\n",
    "train_rows, val_rows = [], []\n",
    "for _, rows in by_cls.items():\n",
    "    n = len(rows)\n",
    "    split = max(1, int(0.85 * n))\n",
    "    train_rows += rows[:split]\n",
    "    val_rows   += rows[split:]\n",
    "\n",
    "train_ds_raw = HFDataset.from_list(train_rows)\n",
    "val_ds_raw   = HFDataset.from_list(val_rows)\n",
    "print(f\"‚úÖ Train/Val sizes: {len(train_ds_raw)} / {len(val_ds_raw)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0x7MhiE-UdwS"
   },
   "source": [
    "## Tokenizer + Custom Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DV9UJEvuUStX",
    "outputId": "82805026-58f0-4aa1-a4c2-edc2c08c4476"
   },
   "outputs": [],
   "source": [
    "# üß† Step 3: Tokenizer + Custom Dataset\n",
    "\n",
    "print(\"üî§ Loading MARBERT tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(config[\"base_model\"])\n",
    "\n",
    "# CHANGE: add 'label_ce' as a single int (main dialect) in __getitem__\n",
    "class AOC_Dataset(TorchDataset):\n",
    "    def __init__(self, dataset, tokenizer):\n",
    "        self.dataset = dataset          # <-- this is an HF dataset (HFDataset)\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataset[idx]\n",
    "\n",
    "        # pull the text; your processed HF rows must have \"sentence\", \"multi_label\", \"regression\"\n",
    "        text = row[\"sentence\"]\n",
    "        if isinstance(text, list):\n",
    "            text = \" \".join(text)\n",
    "\n",
    "        text = clean_arabic_text(text)\n",
    "        enc = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=config[\"max_length\"],\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": enc[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n",
    "            \"labels_cls\": torch.tensor(row[\"multi_label\"], dtype=torch.float),\n",
    "            \"labels_reg\": torch.tensor(row[\"regression\"], dtype=torch.float),\n",
    "        }\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "train_ds = AOC_Dataset(train_ds_raw, tokenizer)\n",
    "val_ds   = AOC_Dataset(val_ds_raw, tokenizer)\n",
    "\n",
    "print(\"‚úÖ Tokenizer loaded and dataset class ready\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WcWH_6KJUhAG"
   },
   "source": [
    "## MARBERT Multi-Task Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZbSNh0kTUjrP",
    "outputId": "95a5cf2b-8d38-48a7-d80e-af269252271c"
   },
   "outputs": [],
   "source": [
    "# üß† Step 4: Multi-Task MARBERT Model\n",
    "\n",
    "print(\"üß† Initializing MARBERT-based model...\")\n",
    "\n",
    "class BertForMultiTask(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(config[\"base_model\"])\n",
    "        hidden_size = self.bert.config.hidden_size\n",
    "        self.classifier = nn.Linear(hidden_size, config[\"num_labels\"])\n",
    "        self.regressor = nn.Linear(hidden_size, config[\"num_labels\"])\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        logits_cls = self.classifier(pooled_output)  # No sigmoid: BCEWithLogits handles that\n",
    "        logits_reg = torch.sigmoid(self.regressor(pooled_output))  # Output between 0‚Äì1\n",
    "        return logits_cls, logits_reg\n",
    "\n",
    "print(\"‚úÖ Model class defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tV6A7TRMUmBM"
   },
   "source": [
    "## DataLoader, Model, Optimizer, and Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dlmyxHH_Upb_",
    "outputId": "fed6dcdb-25b6-4fd6-9941-d591def8cc56",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# üß† Step 5: DataLoader, Model, Optimizer, Losses\n",
    "\n",
    "print(\"üì¶ Creating DataLoader...\")\n",
    "# CHANGE: make train/val loaders\n",
    "train_loader = DataLoader(train_ds, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=config[\"batch_size\"], shuffle=False)\n",
    "\n",
    "print(\"‚öôÔ∏è Initializing model, optimizer, and loss functions...\")\n",
    "model = BertForMultiTask().to(config[\"device\"])\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config[\"learning_rate\"])\n",
    "\n",
    "# CHANGE: loss functions\n",
    "loss_fn_cls = nn.BCEWithLogitsLoss()\n",
    "loss_fn_reg = nn.MSELoss()\n",
    "lambda_reg = 0.2  # start small so classifier fixes itself\n",
    "\n",
    "b = next(iter(train_loader))\n",
    "print({k: v.shape if hasattr(v, \"shape\") else type(v) for k, v in b.items()})\n",
    "\n",
    "print(\"‚úÖ Ready to train\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r9qCBePmUrpe"
   },
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "gKMhGc9jUuWB",
    "outputId": "730fde47-26d8-405a-a009-7cf7f23f2d7d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# üîÅ Step 6: Training Loop (multi-label BCE + small reg weight)\n",
    "\n",
    "lambda_reg = 0.2  # <- keep regression, but don't let it dominate\n",
    "\n",
    "print(\"üöÄ Starting training...\")\n",
    "for epoch in range(config[\"num_epochs\"]):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for step, batch in enumerate(train_loader):   # <-- use train_loader\n",
    "        input_ids = batch[\"input_ids\"].to(config[\"device\"])\n",
    "        attention_mask = batch[\"attention_mask\"].to(config[\"device\"])\n",
    "\n",
    "        # MULTI-LABEL targets (multi-hot)\n",
    "        labels_cls = batch[\"labels_cls\"].to(config[\"device\"])   # <‚Äî CHANGE: use labels_cls (multi-label)\n",
    "        labels_reg = batch[\"labels_reg\"].to(config[\"device\"])\n",
    "\n",
    "        # Forward\n",
    "        logits_cls, logits_reg = model(input_ids, attention_mask)\n",
    "\n",
    "        # Losses (BCE for multilabel; MSE for regression)\n",
    "        loss_cls = loss_fn_cls(logits_cls, labels_cls)          # <‚Äî CHANGE: BCEWithLogitsLoss on multi-hot\n",
    "        loss_reg = loss_fn_reg(logits_reg, labels_reg)\n",
    "        loss = loss_cls + lambda_reg * loss_reg\n",
    "\n",
    "        # Backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if step % 50 == 0:\n",
    "            print(f\"üü° Epoch {epoch+1} Step {step}/{len(train_loader)} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = total_loss / max(1, len(train_loader))\n",
    "    print(f\"‚úÖ Epoch {epoch+1} | Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # ---- quick MULTI-LABEL validation on val_loader ----\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:  # <-- use val_loader\n",
    "            ids  = batch[\"input_ids\"].to(config[\"device\"])\n",
    "            mask = batch[\"attention_mask\"].to(config[\"device\"])\n",
    "\n",
    "            logits_cls, _ = model(ids, mask)\n",
    "            probs = torch.sigmoid(logits_cls).cpu().numpy()  # (B, 5)\n",
    "            preds = (probs >= config[\"classification_threshold\"]).astype(int)\n",
    "\n",
    "            y_true.append(batch[\"labels_cls\"].cpu().numpy())\n",
    "            y_pred.append(preds)\n",
    "\n",
    "    y_true = np.vstack(y_true)\n",
    "    y_pred = np.vstack(y_pred)\n",
    "    print(classification_report(y_true, y_pred, target_names=list(DIALECT2IDX.keys()), digits=3))\n",
    "\n",
    "# Save\n",
    "torch.save(model.state_dict(), \"marbert_sentence_model2.pt\")\n",
    "print(\"üíæ Model saved to marbert_sentence_model2.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5kb6eN32Uwub"
   },
   "source": [
    "## Evaluation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n4zv_R2tU0SM"
   },
   "outputs": [],
   "source": [
    "# üß™ Step 7: Evaluation Loop (multi-label, on VAL set)\n",
    "\n",
    "print(\"üîç Running evaluation...\")\n",
    "model.eval()\n",
    "\n",
    "all_preds_cls, all_labels_cls = [], []\n",
    "all_preds_reg, all_labels_reg = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:  # <-- use val_loader, not 'dataloader'\n",
    "        input_ids = batch[\"input_ids\"].to(config[\"device\"])\n",
    "        attention_mask = batch[\"attention_mask\"].to(config[\"device\"])\n",
    "\n",
    "        # Ground-truth\n",
    "        labels_cls = batch[\"labels_cls\"].cpu().numpy()\n",
    "        labels_reg = batch[\"labels_reg\"].cpu().numpy()\n",
    "\n",
    "        # Forward pass\n",
    "        logits_cls, logits_reg = model(input_ids, attention_mask)\n",
    "\n",
    "        # MULTI-LABEL predictions (sigmoid + threshold)\n",
    "        probs_cls = torch.sigmoid(logits_cls).cpu().numpy()\n",
    "        preds_cls = (probs_cls >= config[\"classification_threshold\"]).astype(int)\n",
    "\n",
    "        # Regression outputs (your reg head already uses sigmoid in the model)\n",
    "        preds_reg = logits_reg.cpu().numpy()\n",
    "\n",
    "        # Collect\n",
    "        all_preds_cls.append(preds_cls)\n",
    "        all_labels_cls.append(labels_cls)\n",
    "        all_preds_reg.append(preds_reg)\n",
    "        all_labels_reg.append(labels_reg)\n",
    "\n",
    "print(\"‚úÖ Evaluation complete\")\n",
    "\n",
    "# Stack to arrays for metrics in block 3\n",
    "all_preds_cls  = np.vstack(all_preds_cls)\n",
    "all_labels_cls = np.vstack(all_labels_cls)\n",
    "all_preds_reg  = np.vstack(all_preds_reg)\n",
    "all_labels_reg = np.vstack(all_labels_reg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6h7JXfrOU2wh"
   },
   "source": [
    "## Full Metrics & Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RbJT7p85U5X-"
   },
   "outputs": [],
   "source": [
    "# üìä Step 8: Full Metrics + Visualizations (multi-label)\n",
    "\n",
    "print(\"üìä Generating metrics and plots...\")\n",
    "\n",
    "# ---- Classification (multi-label) ----\n",
    "print(\"\\nüìÑ Classification Report:\")\n",
    "print(classification_report(all_labels_cls, all_preds_cls, target_names=list(DIALECT2IDX.keys()), digits=3))\n",
    "print(\"‚úÖ Micro F1:\", f1_score(all_labels_cls, all_preds_cls, average='micro'))\n",
    "print(\"‚úÖ Macro F1:\", f1_score(all_labels_cls, all_preds_cls, average='macro'))\n",
    "print(\"üìâ Hamming Loss:\", hamming_loss(all_labels_cls, all_preds_cls))\n",
    "\n",
    "# ---- Regression ----\n",
    "print(\"\\nüìà Regression Metrics:\")\n",
    "mse = mean_squared_error(all_labels_reg, all_preds_reg)\n",
    "mae = mean_absolute_error(all_labels_reg, all_preds_reg)\n",
    "print(f\"‚úÖ MSE: {mse:.4f}\")\n",
    "print(f\"‚úÖ MAE: {mae:.4f}\")\n",
    "\n",
    "# Pearson and Spearman per dialect\n",
    "for i, dialect in enumerate(DIALECT2IDX.keys()):\n",
    "    pearson = pearsonr(all_labels_reg[:, i], all_preds_reg[:, i])[0]\n",
    "    spearman = spearmanr(all_labels_reg[:, i], all_preds_reg[:, i])[0]\n",
    "    print(f\"{dialect:10} | Pearson: {pearson:.4f} | Spearman: {spearman:.4f}\")\n",
    "\n",
    "# ---- Per-dialect confusion matrices (multi-label: 2x2 per class) ----\n",
    "print(\"\\nüìä Confusion Matrices (per dialect, multi-label):\")\n",
    "for i, dialect in enumerate(DIALECT2IDX.keys()):\n",
    "    y_true_bin = all_labels_cls[:, i].astype(int)\n",
    "    y_pred_bin = all_preds_cls[:, i].astype(int)\n",
    "    cm = confusion_matrix(y_true_bin, y_pred_bin, labels=[0,1])\n",
    "    plt.figure(figsize=(3, 3))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\",\n",
    "                xticklabels=[\"Pred 0\", \"Pred 1\"], yticklabels=[\"True 0\", \"True 1\"])\n",
    "    plt.title(f\"Confusion Matrix: {dialect}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ---- Regression scatter ----\n",
    "print(\"üìà Plotting regression scatter plot...\")\n",
    "plt.figure(figsize=(7, 5))\n",
    "for i, dialect in enumerate(DIALECT2IDX.keys()):\n",
    "    plt.scatter(all_labels_reg[:, i], all_preds_reg[:, i], label=dialect, alpha=0.5)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel(\"True Dialectness Score\")\n",
    "plt.ylabel(\"Predicted Score\")\n",
    "plt.title(\"Dialectness Regression: True vs Predicted\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6dkOPXovQ2FV"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "# Inference on the Fineweb2 dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1bkXvadtRSUW"
   },
   "source": [
    "##  Load Your Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AmqcaG6gRQcB",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# 1Ô∏è‚É£ Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"UBC-NLP/MARBERT\")\n",
    "\n",
    "# 2Ô∏è‚É£ Initialize model architecture\n",
    "model = BertForMultiTask()\n",
    "\n",
    "# 3Ô∏è‚É£ Load weights\n",
    "model.load_state_dict(torch.load(\"marbert_sentence_model2.pt\", map_location=\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "# 4Ô∏è‚É£ Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UqC-wcNqGjZt"
   },
   "outputs": [],
   "source": [
    "# gulf dialect\n",
    "# test_sentence = \"\"\"\n",
    "# Ÿàÿ¥ ÿßŸÑÿ≥ÿßŸÑŸÅÿ© Ÿäÿß ŸàŸÑÿØÿü ÿ£ŸÜÿß ÿßŸÖÿ≥ ŸÉŸÜÿ™ ÿ±ÿßŸäÿ≠ ŸÑŸÑÿ®ÿ± ŸÖÿπ ÿßŸÑÿ±ÿ®ÿπÿå Ÿàÿ¥ÿ®ŸëŸäŸÜÿß ÿßŸÑŸÜÿßÿ± Ÿàÿ≥ŸàŸäŸÜÿß ŸÇŸáŸàÿ© Ÿàÿ¥ÿßŸäÿå Ÿàÿ®ÿØŸäŸÜÿß ŸÜÿ≥ŸàŸÑŸÅ ÿπŸÜ ÿ£ŸäÿßŸÖ ÿßŸÑÿ∑Ÿäÿ®ŸäŸÜ.\n",
    "# ŸÖÿ± ÿßŸÑŸàŸÇÿ™ ÿ®ÿ≥ÿ±ÿπÿ©ÿå ŸàŸÉŸÑ Ÿàÿßÿ≠ÿØ ŸÇÿßŸÖ Ÿäÿ≠ŸÉŸä ÿπŸÜ ŸÖŸàÿßŸÇŸÅŸá ŸäŸàŸÖ ŸÉÿßŸÜ ÿ®ÿßŸÑÿ´ÿßŸÜŸàŸäÿ©. ÿ®ÿπÿØŸäŸÜ ÿ∑ŸÇŸäŸÜÿß ÿßŸÑÿπÿ¥ÿßÿ° ÿ±ÿ≤ ŸàŸÑÿ≠ŸÖÿå ŸàŸÑÿß ÿ£ÿ≠ŸÑŸâ.\n",
    "# ÿ±ÿ¨ÿπŸÜÿß ŸÑŸÑÿ®Ÿäÿ™ ŸÇÿ®ŸÑ ÿßŸÑŸÅÿ¨ÿ±ÿå ÿ™ÿπÿ®ÿßŸÜŸäŸÜ ŸÑŸÉŸÜ ŸÖÿ®ÿ≥Ÿàÿ∑ŸäŸÜÿå ŸÖÿß ŸÅŸä ŸÖÿ´ŸÑ ÿ¨ŸÖÿπÿ© ÿßŸÑÿ±ÿ®ÿπ ÿ®ÿßŸÑÿ®ÿ±.\n",
    "# \"\"\"\n",
    "\n",
    "\n",
    "# egyptian dialect\n",
    "# test_sentence = \"\"\"\n",
    "# ÿ£ŸÜÿß ÿ®ÿµÿ±ÿßÿ≠ÿ© ÿ≤ŸáŸÇÿ™ ŸÖŸÜ ÿßŸÑÿ≤ÿ≠ŸÖÿ© ŸàÿßŸÑÿπŸäÿßÿ∑ ÿ®ÿ™ÿßÿπ ŸÉŸÑ ŸäŸàŸÖ ŸÅŸä ÿßŸÑÿ¥ÿ∫ŸÑÿå ŸäÿπŸÜŸä ŸÖÿßŸÅŸäÿ¥ ŸäŸàŸÖ ÿ®ŸäÿπÿØŸëŸä ŸÖŸÜ ÿ∫Ÿäÿ± ŸÖÿß ÿßŸÑŸÖŸàÿßÿµŸÑÿßÿ™ ÿ™ÿπŸÖŸÑ ŸÅŸäÿß ŸÖŸÇŸÑÿ®!\n",
    "# ÿßŸÑŸÜŸáÿßÿ±ÿØÿ© ŸÖÿ´ŸÑÿßŸãÿå ÿµÿ≠Ÿäÿ™ ŸÖÿ™ÿ£ÿÆÿ± ÿπŸÑÿ¥ÿßŸÜ ÿßŸÑŸÉŸáÿ±ÿ®ÿß ŸÇÿ∑ÿπÿ™ÿå ŸàŸÜÿ≤ŸÑÿ™ ÿ£ÿ¨ÿ±Ÿä ÿπŸÑŸâ ÿßŸÑŸÖŸäŸÉÿ±Ÿàÿ®ÿßÿµÿå ÿ®ÿ≥ ÿßŸÑÿ≥ŸàÿßŸÇ ŸÇÿ±ÿ± ŸäÿßÿÆÿØ ÿßŸÑŸÑŸÅÿ© ŸÉŸÑŸáÿß ŸÇÿ®ŸÑ ŸÖÿß ŸäŸàÿµŸÑŸÜŸä.\n",
    "# ÿπÿØŸëŸäŸÜÿß ŸÖŸÜ ÿπŸÜÿØ ÿßŸÑŸÉŸàÿ®ÿ±Ÿä ÿßŸÑŸÑŸä ÿØÿßŸäŸÖŸãÿß ÿ≤ÿ≠ŸÖÿ©ÿå ŸàÿßŸÑŸÜÿßÿ≥ ŸÇÿßÿπÿØÿ© ÿ®ÿ™ÿ≤ÿπŸÇ ŸÑÿ®ÿπÿ∂ ŸÖŸÜ ÿ∫Ÿäÿ± ÿ≥ÿ®ÿ®ÿå ŸàŸÉÿ£ŸÜ ÿßŸÑÿπÿµÿ®Ÿäÿ© ÿ®ŸÇÿ™ ÿ∑ÿ®ŸäÿπŸäÿ© ÿπŸÜÿØŸÜÿß.\n",
    "# ÿ®ÿπÿØ ŸÉÿØŸá ŸàÿµŸÑÿ™ ÿßŸÑÿ¥ÿ∫ŸÑÿå ŸàÿßŸÑŸÖÿØŸäÿ± ÿ£ŸàŸÑ ŸÖÿß ÿ¥ÿßŸÅŸÜŸä ŸÇÿßŸÑŸä: \"ÿßÿ™ÿ£ÿÆÿ±ÿ™ ÿ™ÿßŸÜŸäÿü\" Ÿàÿ£ŸÜÿß ÿ®ÿµŸäÿ™ ŸÑŸá ŸàŸÇŸÑÿ™: \"ŸàÿßŸÑŸÑŸá ÿ∫ÿµÿ® ÿπŸÜŸä\".\n",
    "# ŸäÿπŸÜŸä ŸáŸà ŸÅÿßŸÉÿ± ÿ•ŸÜŸÜÿß ÿ®ŸÜÿ≠ÿ® ÿßŸÑÿ™ÿ£ÿÆŸäÿ±ÿü ŸáŸà ŸÖÿ¥ ÿ≠ÿßÿ≥ÿ≥ ÿ®ÿßŸÑŸÑŸä ÿ®ŸÜÿ¥ŸàŸÅŸá ŸÉŸÑ ŸäŸàŸÖ ŸÅŸä ÿßŸÑÿ¥ÿßÿ±ÿπÿü\n",
    "# ÿßŸÑŸÖŸáŸÖÿå ŸäŸàŸÖ ÿπÿØŸâ ŸàŸäÿß ÿπÿßŸÑŸÖ ÿ®ŸÉÿ±ÿß ŸáŸäÿ®ŸÇŸâ ŸÅŸäŸá ÿ•ŸäŸá!\n",
    "# \"\"\"\n",
    "\n",
    "# levantine dialect\n",
    "# test_sentence = \"\"\"\n",
    "# ŸÖÿ®ÿßÿ±ÿ≠ ŸÜÿ≤ŸÑÿ™ ÿπÿßŸÑÿ≥ŸàŸÇ ŸÖÿπ ÿ±ŸÅŸäŸÇÿ™Ÿäÿå ŸàŸÉÿßŸÜ ŸÅŸä ŸÉÿ™Ÿäÿ± ÿπÿ¨ŸÇÿ© ÿ®ÿ≥ ÿßŸÑÿ¨Ÿà ŸÉÿßŸÜ ÿ≠ŸÑŸà. ÿßÿ¥ÿ™ÿ±ŸäŸÜÿß ÿ¥ŸàŸäÿ© ÿÆÿ∂ÿ±ÿ© ŸàŸÅŸàÿßŸÉŸáÿå Ÿàÿ®ÿπÿØŸäŸÜ ÿ±ÿ≠ŸÜÿß ŸÜÿ¥ÿ±ÿ® ŸÇŸáŸàÿ© ÿ®ÿ¥ÿßÿ±ÿπ ÿßŸÑÿ≠ŸÖÿ±ÿß.\n",
    "# ŸÇÿπÿØŸÜÿß ÿ¥Ÿä ÿ≥ÿßÿπÿ©ÿå ÿ∂ÿ≠ŸÉŸÜÿß Ÿàÿ≠ŸÉŸäŸÜÿß ÿπŸÜ ÿßŸÑÿ¥ÿ∫ŸÑ ŸàÿßŸÑÿ≠Ÿäÿßÿ©. ÿ®ÿπÿØŸäŸÜ ÿ•ÿ¨ÿß ÿ£ÿÆŸàŸáÿß ÿ®ÿ≥Ÿäÿßÿ±ÿ™Ÿá Ÿàÿ£ÿÆÿØŸÜÿß ÿπÿßŸÑÿ®Ÿäÿ™. ÿπŸÜÿ¨ÿØ ŸÉÿßŸÜ ŸÜŸáÿßÿ± ŸÉÿ™Ÿäÿ± ŸÖŸáÿ∂ŸàŸÖ.\n",
    "# \"\"\"\n",
    "\n",
    "# maghrabi dialect\n",
    "test_sentence = \"\"\"\n",
    "ÿßŸÑŸäŸàŸÖ ÿ®ŸÉÿ±Ÿä ŸÖÿ¥Ÿäÿ™ ŸÑŸÑŸÖÿßÿ±ÿ¥Ÿä ŸÜÿ¥ÿ±Ÿä ÿ¥ŸàŸäÿ© ÿÆÿ∂ÿ±ÿ©ÿå ŸÑŸÇŸäÿ™ ÿßŸÑÿØŸÜŸäÿß ÿπÿßŸÖÿ±ÿ© ŸàÿßŸÑŸÜÿßÿ≥ ŸÉŸäÿ™ÿ≥ÿßÿ®ŸÇŸà ÿ®ÿßÿ¥ Ÿäÿ¥ÿ±Ÿà ŸÇÿ®ŸÑ ŸÖÿß ÿ™ÿ≥ÿßŸÑŸä ÿßŸÑÿµÿ®ÿßÿ≠.\n",
    "ÿ¥ÿ±Ÿäÿ™ ŸÖÿ∑Ÿäÿ¥ÿ©ÿå ÿ®ÿµŸÑÿ©ÿå Ÿàÿ¥Ÿä ÿ¥ŸàŸäÿ© ÿØŸäÿßŸÑ ÿßŸÑŸÜÿπŸÜÿßÿπ. ŸàŸÖŸÜ ÿ®ÿπÿØ ŸÖÿ¥Ÿäÿ™ ÿπŸÜÿØ ÿßŸÑÿ¨ÿßÿ±ÿ© ŸÜÿ¥ÿ±ÿ®Ÿà ÿ£ÿ™ÿßŸä ŸàŸÜÿØŸàŸäŸà ÿπŸÑŸâ ŸàŸÑÿßÿØŸÜÿß.\n",
    "ÿßŸÑÿØŸÜŸäÿß ÿ≤ŸàŸäŸÜÿ© ŸàŸÑŸÉŸÜ ÿÆÿßÿµŸÜÿß ŸÜÿ™ŸáŸÑÿßŸà ŸÅÿ®ÿπÿ∂Ÿäÿßÿ™ŸÜÿß.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "cleaned_sentence = clean_arabic_text(test_sentence)\n",
    "\n",
    "encoded = tokenizer(cleaned_sentence, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "\n",
    "# ‚úÖ Fix for unexpected keyword\n",
    "if \"token_type_ids\" in encoded:\n",
    "    del encoded[\"token_type_ids\"]\n",
    "\n",
    "encoded = {k: v.to(device) for k, v in encoded.items()}\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits_cls, logits_reg = model(**encoded)\n",
    "    probs_cls = torch.sigmoid(logits_cls).cpu().numpy()[0]\n",
    "    preds_cls = (probs_cls > 0.5).astype(int)\n",
    "    preds_reg = logits_reg.cpu().numpy()[0]\n",
    "\n",
    "DIALECTS = [\"egyptian\", \"levantine\", \"gulf\", \"maghrebi\", \"msa\"]\n",
    "predicted_dialects = [DIALECTS[i] for i, v in enumerate(preds_cls) if v == 1]\n",
    "\n",
    "print(\"üìù Input:\", test_sentence)\n",
    "print(\"üßº Cleaned:\", cleaned_sentence)\n",
    "print(\"üß† Predicted Dialects:\", predicted_dialects)\n",
    "for i, d in enumerate(DIALECTS):\n",
    "    print(f\" - {d}: {preds_reg[i]:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5A-NYCXfRUr7"
   },
   "source": [
    "## Load & Preprocess FineWeb2 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vRsCdxArRXUH",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# ‚úÖ All Arabic dialect subsets in FineWeb2\n",
    "dialects = [\n",
    "    \"acm_Arab\",  # Iraqi\n",
    "    # \"aeb_Arab\",  # Tunisian\n",
    "    \"apc_Arab\",  # Levantine\n",
    "    \"arb_Arab\",  # MSA\n",
    "    \"arq_Arab\",  # Algerian\n",
    "    \"ars_Arab\",  # Najdi (Saudi)\n",
    "    \"ary_Arab\",  # Moroccan\n",
    "    \"arz_Arab\",  # Egyptian\n",
    "    \"ayp_Arab\",  # North Mesopotamian\n",
    "    \"shu_Arab\",  # Chadian/Sudanese\n",
    "]\n",
    "\n",
    "# üì¶ Stream & sample texts from each dialect\n",
    "samples = []\n",
    "samples_per_dialect = 100  # adjust as needed\n",
    "\n",
    "for dialect in dialects:\n",
    "    print(f\"üì• Sampling from: {dialect}\")\n",
    "    try:\n",
    "        dataset = load_dataset(\"HuggingFaceFW/fineweb-2\", dialect, split=\"train\", streaming=True)\n",
    "        for i, sample in enumerate(dataset):\n",
    "            samples.append(sample[\"text\"])\n",
    "            if i + 1 >= samples_per_dialect:\n",
    "                break\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading {dialect}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JnwrMprPRZIk"
   },
   "source": [
    "## Tokenize the Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-TGktbEQRbPL",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "encoded_batch = tokenizer(samples, padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoded_batch = {k: v.to(device) for k, v in encoded_batch.items()}\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uQE9KD06RcwV"
   },
   "source": [
    "## Run the Model for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6M29vJzKRfAW"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    logits_cls, logits_reg = model(encoded_batch[\"input_ids\"], encoded_batch[\"attention_mask\"])\n",
    "\n",
    "# Convert logits to predictions\n",
    "probs_cls = torch.sigmoid(logits_cls).cpu().numpy()\n",
    "preds_reg = logits_reg.cpu().numpy()\n",
    "\n",
    "# Threshold classification (multi-label)\n",
    "import numpy as np\n",
    "preds_cls = (probs_cls > 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b904tAhVRgii"
   },
   "source": [
    "## Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h9FwowQyRinq",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DIALECTS = [\"egyptian\", \"levantine\", \"gulf\", \"maghrebi\", \"msa\"]\n",
    "\n",
    "for sentence, cls, reg in zip(samples, preds_cls, preds_reg):\n",
    "    sentence = clean_arabic_text(sentence)\n",
    "    predicted_dialects = [DIALECTS[j] for j, val in enumerate(cls) if val == 1]\n",
    "    regression_scores = dict(zip(DIALECTS, reg))\n",
    "\n",
    "    print(\"üìù Text:\", sentence[:60])\n",
    "    print(\"üß† Predicted Dialects:\", predicted_dialects)\n",
    "    for d in DIALECTS:\n",
    "        print(f\" - {d}: {regression_scores[d]:.3f}\")\n",
    "    print(\"=\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "\n",
    "# Check current device\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Current GPU:\", torch.cuda.get_device_name(0))\n",
    "    print(\"GPU Memory (Allocated):\", round(torch.cuda.memory_allocated(0) / 1024**3, 2), \"GB\")\n",
    "    print(\"GPU Memory (Reserved):\", round(torch.cuda.memory_reserved(0) / 1024**3, 2), \"GB\")\n",
    "else:\n",
    "    print(\"Using CPU\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference on Synthetic Data \n",
    "made using ChatGPT, 100 document samples of each dialect (egyptian, levantine, gulf, maghrabi, msa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import torch\n",
    "import re\n",
    "from random import sample as random_sample\n",
    "\n",
    "# ‚úÖ Define dialect mappings\n",
    "DIALECTS = [\"egyptian\", \"levantine\", \"gulf\", \"maghrebi\", \"msa\"]\n",
    "DIALECT2IDX = {d: i for i, d in enumerate(DIALECTS)}\n",
    "IDX2DIALECT = {i: d for i, d in enumerate(DIALECTS)}\n",
    "\n",
    "# üß† Re-initialize the model architecture\n",
    "model = BertForMultiTask().to(config[\"device\"])\n",
    "\n",
    "# üîÅ Load trained weights\n",
    "checkpoint_path = \"marbert_sentence_model2.pt\"\n",
    "model.load_state_dict(torch.load(checkpoint_path, map_location=config[\"device\"]))\n",
    "\n",
    "print(\"‚úÖ Model loaded from checkpoint.\")\n",
    "\n",
    "# üì• Load JSON\n",
    "with open(\"dialect_documents.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    dialect_docs = json.load(f)\n",
    "\n",
    "# üß™ Flatten all samples\n",
    "test_samples = []\n",
    "for label, docs in dialect_docs.items():\n",
    "    for doc in docs:\n",
    "        test_samples.append({\"text\": doc[\"text\"], \"label\": label})\n",
    "\n",
    "def split_sentences(text):\n",
    "    \"\"\"\n",
    "    Splits the input Arabic text into sentences using simple punctuation heuristics.\n",
    "    \"\"\"\n",
    "    # Normalize spacing and remove excessive newlines\n",
    "    text = re.sub(r'\\s+', ' ', text.strip())\n",
    "    \n",
    "    # Split on Arabic and standard sentence-ending punctuation\n",
    "    sentence_enders = re.compile(r'(?<=[.!ÿü\\n])\\s+')\n",
    "    sentences = sentence_enders.split(text)\n",
    "    \n",
    "    # Remove empty or very short entries\n",
    "    return [s.strip() for s in sentences if len(s.strip()) > 5]\n",
    "\n",
    "# ‚úÖ Predict sentence-wise\n",
    "def predict_doc_label_and_score(text):\n",
    "    model.eval()\n",
    "    sentences = split_sentences(text)\n",
    "    sentence_preds = []\n",
    "    sentence_scores = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence = clean_arabic_text(sentence)\n",
    "        encoded = tokenizer(\n",
    "            sentence,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=config[\"max_length\"],\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(config[\"device\"])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits_cls, logits_reg = model(encoded[\"input_ids\"], encoded[\"attention_mask\"])\n",
    "            \n",
    "            # Keep probabilities for ALL classes (multi-label)\n",
    "            probs_cls = torch.sigmoid(logits_cls).cpu().numpy()[0]  # shape (5,)\n",
    "            \n",
    "            # Instead of one-hot argmax, keep full probabilities\n",
    "            sentence_preds.append(probs_cls)\n",
    "            \n",
    "            # Regression as before\n",
    "            pred_reg = torch.sigmoid(logits_reg).cpu().numpy()[0]\n",
    "            sentence_scores.append(pred_reg)\n",
    "\n",
    "\n",
    "    # ‚úÖ Aggregate both classification (votes) and regression (avg score)\n",
    "    votes = np.sum(sentence_preds, axis=0)\n",
    "    avg_reg = np.mean(sentence_scores, axis=0)\n",
    "\n",
    "    boost = np.ones(len(DIALECTS))\n",
    "    boost[DIALECT2IDX[\"maghrebi\"]] = 5.0  # strong boost\n",
    "    boost[DIALECT2IDX[\"gulf\"]] = 1.6      # optional\n",
    "    \n",
    "    combined = (0.75 * votes + 0.25 * avg_reg) * boost\n",
    "    \n",
    "\n",
    "    pred_idx = np.argmax(combined)\n",
    "    pred_label = IDX2DIALECT[pred_idx]\n",
    "\n",
    "    return pred_label, avg_reg.tolist()\n",
    "\n",
    "# üöÄ Predict all\n",
    "predictions = []\n",
    "for sample in tqdm(test_samples, desc=\"üîç Predicting dialects\"):\n",
    "    text = sample[\"text\"]\n",
    "    true = sample[\"label\"]\n",
    "    pred, scores = predict_doc_label_and_score(text)\n",
    "    predictions.append({\n",
    "        \"text\": text,\n",
    "        \"true\": true,\n",
    "        \"pred\": pred,\n",
    "        \"scores\": scores\n",
    "    })\n",
    "\n",
    "# üìå Show 5 samples per dialect\n",
    "grouped = defaultdict(list)\n",
    "for p in predictions:\n",
    "    grouped[p[\"true\"]].append(p)\n",
    "\n",
    "print(\"\\nüìå Sample Predictions (5 per dialect):\")\n",
    "for d in DIALECTS:\n",
    "    print(f\"\\n=== {d.upper()} ===\")\n",
    "    for s in random_sample(grouped[d], 1):\n",
    "        snippet = s[\"text\"][:150].replace('\\n', ' ')\n",
    "        print(f\"True: {s['true']:<10} | Pred: {s['pred']:<10} | Scores: {np.round(s['scores'], 2)}\")\n",
    "        print(f\"Text: {snippet}...\\n\")\n",
    "\n",
    "# üìä Accuracy\n",
    "summary = defaultdict(lambda: {\"correct\": 0, \"wrong\": 0})\n",
    "for p in predictions:\n",
    "    if p[\"true\"] == p[\"pred\"]:\n",
    "        summary[p[\"true\"]][\"correct\"] += 1\n",
    "    else:\n",
    "        summary[p[\"true\"]][\"wrong\"] += 1\n",
    "\n",
    "print(\"\\nüìä Dialect Classification Accuracy:\\n\")\n",
    "print(f\"{'Dialect':<12} {'Correct':>7} {'Wrong':>7} {'Total':>7} {'Accuracy':>9}\")\n",
    "print(\"-\" * 45)\n",
    "for d in DIALECTS:\n",
    "    correct = summary[d][\"correct\"]\n",
    "    wrong = summary[d][\"wrong\"]\n",
    "    total = correct + wrong\n",
    "    acc = 100 * correct / total if total > 0 else 0\n",
    "    print(f\"{d:<12} {correct:>7} {wrong:>7} {total:>7} {acc:>8.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the dialect labels (must match your data)\n",
    "dialects = [\"egyptian\", \"levantine\", \"gulf\", \"maghrebi\", \"msa\"]\n",
    "\n",
    "# Extract true and predicted labels from your prediction results\n",
    "y_true = [p[\"true\"] for p in predictions]\n",
    "y_pred = [p[\"pred\"] for p in predictions]\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred, labels=dialects)\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=dialects, yticklabels=dialects)\n",
    "plt.xlabel(\"Predicted Dialect\")\n",
    "plt.ylabel(\"Actual Dialect\")\n",
    "plt.title(\"Dialect Classification Confusion Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n‚ùå Misclassified Samples:\\n\")\n",
    "\n",
    "for item in predictions:\n",
    "    if item[\"true\"] != item[\"pred\"]:\n",
    "        text_snippet = item[\"text\"][:200].replace(\"\\n\", \" \")\n",
    "        print(f\"True: {item['true']:<10} | Predicted: {item['pred']:<10} | Text: {text_snippet}...\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (Arabic Dialects)",
   "language": "python",
   "name": "arabic-dialects"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0c4590aae9f8422d8d244d6468f70c6b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "128f9178b7094046901c7a538c3f31e6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "149238dd7ed54a1ca43c68688e3e7e2f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "26074db8d4454fa490e6652f8fe9f6bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7cab2d2b099c4cc4a498ee558741ae47",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_41daca42de144559ad4afb3e2140b9a6",
      "value": "Map:‚Äá100%"
     }
    },
    "41daca42de144559ad4afb3e2140b9a6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4b0bf0a7ec3e490cb696b9647a5d7e11": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6ef81a07d70e44d79e0815790ff9bf42": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0c4590aae9f8422d8d244d6468f70c6b",
      "max": 102886,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_128f9178b7094046901c7a538c3f31e6",
      "value": 102886
     }
    },
    "77726d0a11ae4ae193f6bdc0d8a56176": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4b0bf0a7ec3e490cb696b9647a5d7e11",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_cfb9e924ff994e69ab0f5990e8df4f51",
      "value": "‚Äá102886/102886‚Äá[00:27&lt;00:00,‚Äá4147.65‚Äáexamples/s]"
     }
    },
    "7cab2d2b099c4cc4a498ee558741ae47": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "861f69ab7b184e368f4bb25095ef48b9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_26074db8d4454fa490e6652f8fe9f6bc",
       "IPY_MODEL_6ef81a07d70e44d79e0815790ff9bf42",
       "IPY_MODEL_77726d0a11ae4ae193f6bdc0d8a56176"
      ],
      "layout": "IPY_MODEL_149238dd7ed54a1ca43c68688e3e7e2f"
     }
    },
    "cfb9e924ff994e69ab0f5990e8df4f51": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
